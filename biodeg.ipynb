{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabalho realizado por: \n",
    "- Daniel Luís, 56362\n",
    "- João Gonçalo Santos, 57103\n",
    "- Paulo Bolinhas, 56300\n",
    "- Rui Martins, 56283"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "# Load the dataset\n",
    "biodeg_data = pd.read_csv('biodegradable_a.csv', sep=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazer a respetiva divisão das colunas entre valores contínuos e categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns_names = biodeg_data.columns\n",
    "all_data_values = biodeg_data.values\n",
    "\n",
    "categorical_cols = [23,24, 28, 41]\n",
    "continuous_cols = [col for col in range(42) if col not in categorical_cols]\n",
    "\n",
    "#data by type\n",
    "categorical_data = biodeg_data.iloc[:, categorical_cols]\n",
    "continuous_data = biodeg_data.iloc[:, continuous_cols]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lidar com os valores em falta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise das colunas para determinar a quantidade de valores em falta, para escolher uma estratégia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines: 4564\n",
      "Column Psi_i_1d has 23.4 % outliers.\n",
      "Column SpMax_B has 29.75 % missing values.\n"
     ]
    }
   ],
   "source": [
    "q1 = continuous_data.quantile(0.25)\n",
    "q3 = continuous_data.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "outliers = []\n",
    "\n",
    "num_lines = len(continuous_data)\n",
    "print(\"Total number of lines:\", num_lines)\n",
    "\n",
    "missing_values = []\n",
    "for col in continuous_data.columns:\n",
    "    # calcular o numero de valores em falta\n",
    "    num_missing = continuous_data[col].isnull().sum()\n",
    "    missing_values.append(num_missing)\n",
    "    pct_missing = num_missing / num_lines * 100\n",
    "    pct_missing = round(pct_missing, 2)\n",
    "    if (pct_missing > 20):\n",
    "        print(\"Column\", col, \"has\", pct_missing, \"% missing values.\")\n",
    "\n",
    "    # calcular o numero de outliers\n",
    "    num_outliers = ((continuous_data[col] < (q1[col] - 1.5 * iqr[col])) | (continuous_data[col] > (q3[col] + 1.5 * iqr[col]))).sum()\n",
    "    outliers.append(num_outliers)\n",
    "    pct_outliers = num_outliers / num_lines * 100\n",
    "    pct_outliers = round(pct_outliers, 2)\n",
    "    if (pct_outliers > 20):\n",
    "        print(\"Column\", col, \"has\", pct_outliers, \"% outliers.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica-se que apenas duas colunas ultrapassam as métricas analisadas, por isso, o algoritmo a utilizar para lidar com valores em falta de dados contínuos é o mean."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função pré-definida para calcular e devolver as várias métricas de avaliação relevantes relativos a um problema de classficação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, matthews_corrcoef, precision_score, recall_score\n",
    "\n",
    "def present_reg_statistics(y_test, preds):\n",
    "    print(\"The Precision is: %7.4f\" % precision_score(y_test, preds, pos_label='RB'))\n",
    "    print(\"The Recall is: %7.4f\" % recall_score(y_test, preds, pos_label='RB'))\n",
    "    print(\"The F1 score is: %7.4f\" % f1_score(y_test, preds, pos_label='RB'))\n",
    "    print(\"The Matthews correlation coefficient is: %7.4f\" % matthews_corrcoef(y_test, preds))\n",
    "    print(\"This is the Confusion Matrix\")\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, preds)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise dos modelos de scaling para KNN + Simple Cross Validation vs K Fold Validation + Tunning de hiperparâmetros + Comportamento do classificador usando todos os dados disponíveis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliar qual o scaler que obtém melhores resultados para o classificador KNeighborsClassifier, testando com Simple Cross Validation vs K Fold Validation, e, também, fazendo uso da técnica de tunning de hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ------------------ VALIDATOR K ------------------\n",
      "\n",
      "\n",
      " --- SCALING WITH StandardScaler:\n",
      "\n",
      "\n",
      " - KNN with uniform\n",
      "\n",
      " NN - 3\n",
      "The Precision is:  0.9673\n",
      "The Recall is:  0.9872\n",
      "The F1 score is:  0.9772\n",
      "The Matthews correlation coefficient is:  0.8344\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  53   13\n",
      "1   5  385\n",
      "Accuracy: 0.9605263157894737\n",
      "\n",
      " NN - 5\n",
      "The Precision is:  0.9699\n",
      "The Recall is:  0.9923\n",
      "The F1 score is:  0.9810\n",
      "The Matthews correlation coefficient is:  0.8622\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  54   12\n",
      "1   3  387\n",
      "Accuracy: 0.9671052631578947\n",
      "\n",
      " NN - 7\n",
      "The Precision is:  0.9650\n",
      "The Recall is:  0.9897\n",
      "The F1 score is:  0.9772\n",
      "The Matthews correlation coefficient is:  0.8336\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  52   14\n",
      "1   4  386\n",
      "Accuracy: 0.9605263157894737\n",
      "\n",
      " - KNN with distance\n",
      "\n",
      " NN - 3\n",
      "The Precision is:  0.9673\n",
      "The Recall is:  0.9846\n",
      "The F1 score is:  0.9759\n",
      "The Matthews correlation coefficient is:  0.8257\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  53   13\n",
      "1   6  384\n",
      "Accuracy: 0.9583333333333334\n",
      "\n",
      " NN - 5\n",
      "The Precision is:  0.9698\n",
      "The Recall is:  0.9897\n",
      "The F1 score is:  0.9797\n",
      "The Matthews correlation coefficient is:  0.8531\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  54   12\n",
      "1   4  386\n",
      "Accuracy: 0.9649122807017544\n",
      "\n",
      " NN - 7\n",
      "The Precision is:  0.9675\n",
      "The Recall is:  0.9923\n",
      "The F1 score is:  0.9797\n",
      "The Matthews correlation coefficient is:  0.8526\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  53   13\n",
      "1   3  387\n",
      "Accuracy: 0.9649122807017544\n",
      "\n",
      " --- SCALING WITH MinMaxScaler:\n",
      "\n",
      "\n",
      " - KNN with uniform\n",
      "\n",
      " NN - 3\n",
      "The Precision is:  0.9699\n",
      "The Recall is:  0.9923\n",
      "The F1 score is:  0.9810\n",
      "The Matthews correlation coefficient is:  0.8622\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  54   12\n",
      "1   3  387\n",
      "Accuracy: 0.9671052631578947\n",
      "\n",
      " NN - 5\n",
      "The Precision is:  0.9675\n",
      "The Recall is:  0.9923\n",
      "The F1 score is:  0.9797\n",
      "The Matthews correlation coefficient is:  0.8526\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  53   13\n",
      "1   3  387\n",
      "Accuracy: 0.9649122807017544\n",
      "\n",
      " NN - 7\n",
      "The Precision is:  0.9628\n",
      "The Recall is:  0.9949\n",
      "The F1 score is:  0.9786\n",
      "The Matthews correlation coefficient is:  0.8427\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  51   15\n",
      "1   2  388\n",
      "Accuracy: 0.9627192982456141\n",
      "\n",
      " - KNN with distance\n",
      "\n",
      " NN - 3\n",
      "The Precision is:  0.9698\n",
      "The Recall is:  0.9897\n",
      "The F1 score is:  0.9797\n",
      "The Matthews correlation coefficient is:  0.8531\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  54   12\n",
      "1   4  386\n",
      "Accuracy: 0.9649122807017544\n",
      "\n",
      " NN - 5\n",
      "The Precision is:  0.9699\n",
      "The Recall is:  0.9923\n",
      "The F1 score is:  0.9810\n",
      "The Matthews correlation coefficient is:  0.8622\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  54   12\n",
      "1   3  387\n",
      "Accuracy: 0.9671052631578947\n",
      "\n",
      " NN - 7\n",
      "The Precision is:  0.9652\n",
      "The Recall is:  0.9949\n",
      "The F1 score is:  0.9798\n",
      "The Matthews correlation coefficient is:  0.8523\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  52   14\n",
      "1   2  388\n",
      "Accuracy: 0.9649122807017544\n",
      "\n",
      " --- SCALING WITH PowerTransformer:\n",
      "\n",
      "\n",
      " - KNN with uniform\n",
      "\n",
      " NN - 3\n",
      "The Precision is:  0.9797\n",
      "The Recall is:  0.9923\n",
      "The F1 score is:  0.9860\n",
      "The Matthews correlation coefficient is:  0.9003\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  58    8\n",
      "1   3  387\n",
      "Accuracy: 0.9758771929824561\n",
      "\n",
      " NN - 5\n",
      "The Precision is:  0.9797\n",
      "The Recall is:  0.9923\n",
      "The F1 score is:  0.9860\n",
      "The Matthews correlation coefficient is:  0.9003\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  58    8\n",
      "1   3  387\n",
      "Accuracy: 0.9758771929824561\n",
      "\n",
      " NN - 7\n",
      "The Precision is:  0.9627\n",
      "The Recall is:  0.9923\n",
      "The F1 score is:  0.9773\n",
      "The Matthews correlation coefficient is:  0.8331\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  51   15\n",
      "1   3  387\n",
      "Accuracy: 0.9605263157894737\n",
      "\n",
      " - KNN with distance\n",
      "\n",
      " NN - 3\n",
      "The Precision is:  0.9772\n",
      "The Recall is:  0.9897\n",
      "The F1 score is:  0.9834\n",
      "The Matthews correlation coefficient is:  0.8820\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  57    9\n",
      "1   4  386\n",
      "Accuracy: 0.9714912280701754\n",
      "\n",
      " NN - 5\n",
      "The Precision is:  0.9797\n",
      "The Recall is:  0.9923\n",
      "The F1 score is:  0.9860\n",
      "The Matthews correlation coefficient is:  0.9003\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  58    8\n",
      "1   3  387\n",
      "Accuracy: 0.9758771929824561\n",
      "\n",
      " NN - 7\n",
      "The Precision is:  0.9699\n",
      "The Recall is:  0.9923\n",
      "The F1 score is:  0.9810\n",
      "The Matthews correlation coefficient is:  0.8622\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  54   12\n",
      "1   3  387\n",
      "Accuracy: 0.9671052631578947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "scalers = {\n",
    "    \"StandardScaler\": StandardScaler(), \n",
    "    \"MinMaxScaler\": MinMaxScaler(), \n",
    "    \"PowerTransformer\": PowerTransformer()\n",
    "}\n",
    "\n",
    "#Foi retirado o Simple Cross Validator, de forma a cumprir o requisito imposto relativo às 12páginas máx\n",
    "#Basta decomentar a linha abaixo e comentar a seguinte para verificar simple cross validation.\n",
    "\n",
    "# validators = [\"Simple\",\"K\"]\n",
    "validators = [\"K\"]\n",
    "\n",
    "for validator in validators:\n",
    "    print(\"\\n\",f\"------------------ VALIDATOR {validator} ------------------\\n\")\n",
    "\n",
    "    X = biodeg_data.values[:, :-1]\n",
    "    y = biodeg_data[\"Biodegradable\"].values\n",
    "\n",
    "    if validator == \"Simple\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "    else:\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=23)\n",
    "        kf.get_n_splits(X)\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        print(\"\\n\",f\"--- SCALING WITH {scaler_name}:\\n\")\n",
    "            \n",
    "        #Scale only the continous data\n",
    "        Xt_train_cont = pd.DataFrame(scaler.fit_transform(X_train[:, continuous_cols]), columns=columns_names[continuous_cols])\n",
    "        Xt_test_cont = pd.DataFrame(scaler.transform(X_test[:, continuous_cols]), columns=columns_names[continuous_cols])\n",
    "    \n",
    "        #Get the categorical data (not scaled)\n",
    "        Xt_train_cat = pd.DataFrame(X_train[:, categorical_cols[:-1]], columns=columns_names[categorical_cols[:-1]])\n",
    "        Xt_test_cat = pd.DataFrame(X_test[:, categorical_cols[:-1]], columns=columns_names[categorical_cols[:-1]])\n",
    "\n",
    "        # Join continuous and categorical data\n",
    "        Xt_train = pd.concat([Xt_train_cont, Xt_train_cat], axis=1)\n",
    "        Xt_test = pd.concat([Xt_test_cont, Xt_test_cat], axis=1)\n",
    "        \n",
    "        #Original order with new data\n",
    "        # Get the original column order\n",
    "        original_column_order = list(biodeg_data.columns[:-1])\n",
    "        \n",
    "        # Reorder the columns in the preprocessed data to match the original order\n",
    "        Xt_train = Xt_train[original_column_order]\n",
    "        Xt_test = Xt_test[original_column_order]\n",
    "        \n",
    "        #Re-convert in numpy.ndarray\n",
    "        Xt_train = Xt_train.values\n",
    "        Xt_test = Xt_test.values\n",
    "\n",
    "        #Handle missing values\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        Xt_train_imp_cont = imputer.fit_transform(Xt_train[:, continuous_cols])\n",
    "        Xt_test_imp_cont = imputer.fit_transform(Xt_test[:, continuous_cols])\n",
    "        \n",
    "        imputer = SimpleImputer(strategy='most_frequent')\n",
    "        Xt_train_imp_cat = imputer.fit_transform(Xt_train[:, categorical_cols[:-1]])\n",
    "        Xt_test_imp_cat = imputer.fit_transform(Xt_test[:, categorical_cols[:-1]])\n",
    "        \n",
    "        # concatenate the continuous and categorical arrays\n",
    "        Xt_train_imp = np.concatenate([Xt_train_imp_cont, Xt_train_imp_cat], axis=1)\n",
    "        Xt_test_imp = np.concatenate([Xt_test_imp_cont, Xt_test_imp_cat], axis=1)\n",
    "\n",
    "        # retrieve the original column order\n",
    "        original_order = np.concatenate([continuous_cols, categorical_cols[:-1]])\n",
    "        sorted_indices = np.argsort(original_order)\n",
    "\n",
    "        #reorder the concatenated arrays into the original order \n",
    "        Xt_train = Xt_train_imp[:, sorted_indices]\n",
    "        Xt_test = Xt_test_imp[:, sorted_indices]\n",
    "        \n",
    "        #----------------\n",
    "        # define the values of k to try\n",
    "        k_values = range(3, 8, 2)\n",
    "\n",
    "        # define the types of weights to use\n",
    "        weights_list = [\"uniform\", \"distance\"]\n",
    "\n",
    "        for weights in weights_list:\n",
    "            print(\"\\n\", f\"- KNN with {weights}\")\n",
    "            for k in k_values:\n",
    "                knn = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
    "                # fit & predict\n",
    "                knn.fit(Xt_train, y_train)\n",
    "                preds = knn.predict(Xt_test)\n",
    "\n",
    "                # print evaluation metrics\n",
    "                print(\"\\n\", f\"NN - {k}\")\n",
    "                accuracy = accuracy_score(y_test, preds)\n",
    "                present_reg_statistics(y_test, preds)\n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\paulo\\OneDrive\\Ambiente de Trabalho\\LEI\\3º ano\\2º semestre\\EC\\Projeto2\\BioDegradable-MachineLearning\\BioDegradable-MachineLearning\\biodeg.ipynb Cell 15\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tree_entropia \u001b[39m=\u001b[39m DecisionTreeClassifier(criterion\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tree_entropia\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_pred_entropia \u001b[39m=\u001b[39m tree_entropia\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tree_gini \u001b[39m=\u001b[39m DecisionTreeClassifier(criterion\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgini\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\tree\\_classes.py:186\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    184\u001b[0m check_X_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mDTYPE, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    185\u001b[0m check_y_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 186\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    187\u001b[0m     X, y, validate_separately\u001b[39m=\u001b[39;49m(check_X_params, check_y_params)\n\u001b[0;32m    188\u001b[0m )\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    190\u001b[0m     X\u001b[39m.\u001b[39msort_indices()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:560\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_X_params:\n\u001b[0;32m    559\u001b[0m     check_X_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params}\n\u001b[1;32m--> 560\u001b[0m X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params)\n\u001b[0;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_y_params:\n\u001b[0;32m    562\u001b[0m     check_y_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "tree_entropia = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "tree_entropia.fit(Xt_train, y_train)\n",
    "y_pred_entropia = tree_entropia.predict(Xt_test)\n",
    "\n",
    "tree_gini = DecisionTreeClassifier(criterion=\"gini\")\n",
    "tree_gini.fit(Xt_train, y_train)\n",
    "y_pred_gini = tree_gini.predict(Xt_test)\n",
    "\n",
    "present_reg_statistics(y_test, y_pred_entropia)\n",
    "present_reg_statistics(y_test, y_pred_gini)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "#### Correlation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top X atributos mais correlacionados à variàvel y - Variante: Correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "N = Xt_train.shape[0]\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "mapping = {'RB': 0, 'NRB': 1}\n",
    "\n",
    "# Use the map method to replace the values in y_train\n",
    "y_train_mapped = np.array([mapping[label] for label in y_train])\n",
    "y_test_mapped = np.array([mapping[label] for label in y_test])\n",
    "\n",
    "# converter y para float\n",
    "y_train_float = y_train_mapped.astype(float)\n",
    "y_test_float = y_test_mapped.astype(float)\n",
    "\n",
    "#append the y to the X matrix\n",
    "v=np.hstack((y_train_float.reshape((N,1)), Xt_train))\n",
    "\n",
    "# converter de object para float64\n",
    "v = v.astype(np.float64)\n",
    "\n",
    "#compute and view the correlation matrix (note that we are interested only on the first column (0) \n",
    "#which shows the correlation between each variable and the label y.\n",
    "corr_matrix_y = np.corrcoef(v.T)[0, 1:]\n",
    "\n",
    "#38 - tested and got the best values\n",
    "top_X_corr_vars = np.argsort(abs(corr_matrix_y))[::-1][:38]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise Feature selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top X atributos mais correlacionados à variàvel y - Variante: Stepwise methods, com duas variantes, nomeadamente direction=\"forward\" vs \"backward\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\paulo\\OneDrive\\Ambiente de Trabalho\\LEI\\3º ano\\2º semestre\\EC\\Projeto2\\BioDegradable-MachineLearning\\BioDegradable-MachineLearning\\biodeg.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m lmr \u001b[39m=\u001b[39m LinearRegression()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m sfs_forward \u001b[39m=\u001b[39m SequentialFeatureSelector(lmr, n_features_to_select\u001b[39m=\u001b[39m\u001b[39m39\u001b[39m, direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m'\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m sfs_forward\u001b[39m.\u001b[39;49mfit(Xt_train, y_train_float)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# get the relevant columns\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m features_forward \u001b[39m=\u001b[39m sfs_forward\u001b[39m.\u001b[39mget_support()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_selection\\_sequential.py:268\u001b[0m, in \u001b[0;36mSequentialFeatureSelector.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    266\u001b[0m is_auto_select \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_to_select \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_iterations):\n\u001b[1;32m--> 268\u001b[0m     new_feature_idx, new_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_best_new_feature_score(\n\u001b[0;32m    269\u001b[0m         cloned_estimator, X, y, current_mask\n\u001b[0;32m    270\u001b[0m     )\n\u001b[0;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m is_auto_select \u001b[39mand\u001b[39;00m ((new_score \u001b[39m-\u001b[39m old_score) \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol):\n\u001b[0;32m    272\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_selection\\_sequential.py:299\u001b[0m, in \u001b[0;36mSequentialFeatureSelector._get_best_new_feature_score\u001b[1;34m(self, estimator, X, y, current_mask)\u001b[0m\n\u001b[0;32m    297\u001b[0m         candidate_mask \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mcandidate_mask\n\u001b[0;32m    298\u001b[0m     X_new \u001b[39m=\u001b[39m X[:, candidate_mask]\n\u001b[1;32m--> 299\u001b[0m     scores[feature_idx] \u001b[39m=\u001b[39m cross_val_score(\n\u001b[0;32m    300\u001b[0m         estimator,\n\u001b[0;32m    301\u001b[0m         X_new,\n\u001b[0;32m    302\u001b[0m         y,\n\u001b[0;32m    303\u001b[0m         cv\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv,\n\u001b[0;32m    304\u001b[0m         scoring\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscoring,\n\u001b[0;32m    305\u001b[0m         n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    306\u001b[0m     )\u001b[39m.\u001b[39mmean()\n\u001b[0;32m    307\u001b[0m new_feature_idx \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(scores, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m feature_idx: scores[feature_idx])\n\u001b[0;32m    308\u001b[0m \u001b[39mreturn\u001b[39;00m new_feature_idx, scores[new_feature_idx]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_base.py:656\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    648\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m    649\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39maccept_sparse, y_numeric\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, multi_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    650\u001b[0m )\n\u001b[0;32m    652\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    653\u001b[0m     sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype, only_non_negative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    654\u001b[0m )\n\u001b[1;32m--> 656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[39m=\u001b[39m _preprocess_data(\n\u001b[0;32m    657\u001b[0m     X,\n\u001b[0;32m    658\u001b[0m     y,\n\u001b[0;32m    659\u001b[0m     fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[0;32m    660\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_X,\n\u001b[0;32m    661\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    662\u001b[0m )\n\u001b[0;32m    664\u001b[0m \u001b[39m# Sample weight can be implemented via a simple rescaling.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m X, y, sample_weight_sqrt \u001b[39m=\u001b[39m _rescale_data(X, y, sample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_base.py:276\u001b[0m, in \u001b[0;36m_preprocess_data\u001b[1;34m(X, y, fit_intercept, normalize, copy, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m         X_scale \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m--> 276\u001b[0m     y_offset \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49maverage(y, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, weights\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    277\u001b[0m     y \u001b[39m=\u001b[39m y \u001b[39m-\u001b[39m y_offset\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\function_base.py:546\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[0;32m    543\u001b[0m     wgt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mbroadcast_to(wgt, (a\u001b[39m.\u001b[39mndim\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m wgt\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    544\u001b[0m     wgt \u001b[39m=\u001b[39m wgt\u001b[39m.\u001b[39mswapaxes(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis)\n\u001b[1;32m--> 546\u001b[0m scl \u001b[39m=\u001b[39m wgt\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mresult_dtype, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkeepdims_kw)\n\u001b[0;32m    547\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39many(scl \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m):\n\u001b[0;32m    548\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mZeroDivisionError\u001b[39;00m(\n\u001b[0;32m    549\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWeights sum to zero, can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be normalized\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "N,M=Xt_train.shape\n",
    "#----Forward----\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "lmr = LinearRegression()\n",
    "sfs_forward = SequentialFeatureSelector(lmr, n_features_to_select=39, direction='forward', cv=5)\n",
    "sfs_forward.fit(Xt_train, y_train_float)\n",
    "# get the relevant columns\n",
    "features_forward = sfs_forward.get_support()\n",
    "Features_selected_forward =np.arange(M)[features_forward]\n",
    "print(\"Forward - The features selected are columns: \", Features_selected_forward)\n",
    "\n",
    "nXt_train_stepWise_forward=sfs_forward.transform(Xt_train)\n",
    "nXt_test_stepWise_forward=sfs_forward.transform(Xt_test)\n",
    "\n",
    "#----Backward----\n",
    "lmr = LinearRegression()\n",
    "sfs_backward = SequentialFeatureSelector(lmr, n_features_to_select=39, direction='backward', cv=5)\n",
    "sfs_backward.fit(Xt_train, y_train_float)\n",
    "# get the relevant columns\n",
    "features_backward = sfs_backward.get_support()\n",
    "Features_selected_backward =np.arange(M)[features_backward]\n",
    "print(\"Backward - The features selected are columns: \", Features_selected_backward)\n",
    "\n",
    "nXt_train_stepWise_backward=sfs_backward.transform(Xt_train)\n",
    "nXt_test_stepWise_backward=sfs_backward.transform(Xt_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar se o uso de feature selection é positivo, ou não, relativamente ao uso de todos os dados, para o classificador KNeighborsClassifier com utilização da configuração utilizada que deu origem, até aqui, à \"melhor\" performance obtida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with Correlation Feature selection \n",
      "\n",
      "The Precision is:  0.9773\n",
      "The Recall is:  0.9923\n",
      "The F1 score is:  0.9847\n",
      "The Matthews correlation coefficient is:  0.8909\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  57    9\n",
      "1   3  387\n",
      "Accuracy:  0.9736842105263158 \n",
      "\n",
      "KNN with Forward Stepwise Feature selection \n",
      "\n",
      "The Precision is:  0.9797\n",
      "The Recall is:  0.9923\n",
      "The F1 score is:  0.9860\n",
      "The Matthews correlation coefficient is:  0.9003\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  58    8\n",
      "1   3  387\n",
      "Accuracy:  0.9758771929824561 \n",
      "\n",
      "KNN with Backward Stepwise Feature selection \n",
      "\n",
      "The Precision is:  0.9797\n",
      "The Recall is:  0.9897\n",
      "The F1 score is:  0.9847\n",
      "The Matthews correlation coefficient is:  0.8916\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  58    8\n",
      "1   4  386\n",
      "Accuracy:  0.9736842105263158 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#Best scale\n",
    "X = biodeg_data.values[:, :-1]\n",
    "y = biodeg_data[\"Biodegradable\"].values\n",
    "\n",
    "#Xt_train, Xt_test are ready to use, beacause are scaled with the last and best performance scaler PowerTransformer() + cross validation\n",
    "Xt_train_topX_corr = Xt_train[:, top_X_corr_vars]\n",
    "Xt_test_topX_corr = Xt_test[:, top_X_corr_vars]\n",
    "\n",
    "#Correlation & Step Wise\n",
    "feature_selection_option = [[Xt_train_topX_corr, Xt_test_topX_corr, \"KNN with Correlation Feature selection\"], \n",
    "                            [nXt_train_stepWise_forward, nXt_test_stepWise_forward, \"KNN with Forward Stepwise Feature selection\"],\n",
    "                            [nXt_train_stepWise_backward, nXt_test_stepWise_backward, \"KNN with Backward Stepwise Feature selection\"]]\n",
    "\n",
    "for option in feature_selection_option:\n",
    "    print(option[2], \"\\n\")\n",
    "    #Best config of KNN (n_neighbors=5, weights=\"uniform\")  ------ AQUI METER MELHOR CONFIG (VER COM TABELA EXCEL)\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights=\"uniform\")\n",
    "    # Fit & predict with the features selected as the current option\n",
    "    knn.fit(option[0], y_train)\n",
    "    preds = knn.predict(option[1])\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "    # Print evaluation metrics for the current option\n",
    "    present_reg_statistics(y_test, preds)\n",
    "    print(\"Accuracy: \", accuracy, \"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outros modelos classificadores e comparação entre todas as features ou as X melhores selecionadas ##\n",
    "Logistic Regression, Decision Tree e SVC com utilização da técnica de tunning de hiperparâmetros para cada um dos classificadores, individualmente. Utilização, para posterior comparação, de todas as variantes de features, isto é, para todas elas, para features selecionadas via métodos de Correlation e features selecionadas via Stepwise methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Using  All data ----\n",
      "\n",
      "- Logistic Regression:\n",
      "Best Hyperparameters: {'C': 10}\n",
      "The Precision is:  0.9529\n",
      "The Recall is:  0.9846\n",
      "The F1 score is:  0.9685\n",
      "The Matthews correlation coefficient is:  0.7649\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  47   19\n",
      "1   6  384\n",
      "Accuracy: 0.9451754385964912 \n",
      "\n",
      "- Decision Tree:\n",
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_split': 10}\n",
      "The Precision is:  0.9721\n",
      "The Recall is:  0.9821\n",
      "The F1 score is:  0.9770\n",
      "The Matthews correlation coefficient is:  0.8370\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  55   11\n",
      "1   7  383\n",
      "Accuracy: 0.9605263157894737 \n",
      "\n",
      "- SVC:\n",
      "Best Hyperparameters: {'C': 1, 'gamma': 0.1}\n",
      "The Precision is:  0.9847\n",
      "The Recall is:  0.9897\n",
      "The F1 score is:  0.9872\n",
      "The Matthews correlation coefficient is:  0.9104\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  60    6\n",
      "1   4  386\n",
      "Accuracy: 0.9780701754385965 \n",
      "\n",
      "---- Using  Correlation Feature selection ----\n",
      "\n",
      "- Logistic Regression:\n",
      "Best Hyperparameters: {'C': 1}\n",
      "The Precision is:  0.9529\n",
      "The Recall is:  0.9846\n",
      "The F1 score is:  0.9685\n",
      "The Matthews correlation coefficient is:  0.7649\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  47   19\n",
      "1   6  384\n",
      "Accuracy: 0.9451754385964912 \n",
      "\n",
      "- Decision Tree:\n",
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_split': 8}\n",
      "The Precision is:  0.9770\n",
      "The Recall is:  0.9821\n",
      "The F1 score is:  0.9795\n",
      "The Matthews correlation coefficient is:  0.8566\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  57    9\n",
      "1   7  383\n",
      "Accuracy: 0.9649122807017544 \n",
      "\n",
      "- SVC:\n",
      "Best Hyperparameters: {'C': 1, 'gamma': 0.1}\n",
      "The Precision is:  0.9872\n",
      "The Recall is:  0.9897\n",
      "The F1 score is:  0.9885\n",
      "The Matthews correlation coefficient is:  0.9198\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  61    5\n",
      "1   4  386\n",
      "Accuracy: 0.9802631578947368 \n",
      "\n",
      "---- Using  Forward Stepwise Feature selection ----\n",
      "\n",
      "- Logistic Regression:\n",
      "Best Hyperparameters: {'C': 10}\n",
      "The Precision is:  0.9529\n",
      "The Recall is:  0.9846\n",
      "The F1 score is:  0.9685\n",
      "The Matthews correlation coefficient is:  0.7649\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  47   19\n",
      "1   6  384\n",
      "Accuracy: 0.9451754385964912 \n",
      "\n",
      "- Decision Tree:\n",
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_split': 8}\n",
      "The Precision is:  0.9673\n",
      "The Recall is:  0.9846\n",
      "The F1 score is:  0.9759\n",
      "The Matthews correlation coefficient is:  0.8257\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  53   13\n",
      "1   6  384\n",
      "Accuracy: 0.9583333333333334 \n",
      "\n",
      "- SVC:\n",
      "Best Hyperparameters: {'C': 1, 'gamma': 0.1}\n",
      "The Precision is:  0.9847\n",
      "The Recall is:  0.9897\n",
      "The F1 score is:  0.9872\n",
      "The Matthews correlation coefficient is:  0.9104\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  60    6\n",
      "1   4  386\n",
      "Accuracy: 0.9780701754385965 \n",
      "\n",
      "---- Using  Backward Stepwise Feature selection ----\n",
      "\n",
      "- Logistic Regression:\n",
      "Best Hyperparameters: {'C': 10}\n",
      "The Precision is:  0.9552\n",
      "The Recall is:  0.9846\n",
      "The F1 score is:  0.9697\n",
      "The Matthews correlation coefficient is:  0.7752\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  48   18\n",
      "1   6  384\n",
      "Accuracy: 0.9473684210526315 \n",
      "\n",
      "- Decision Tree:\n",
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_split': 10}\n",
      "The Precision is:  0.9672\n",
      "The Recall is:  0.9821\n",
      "The F1 score is:  0.9746\n",
      "The Matthews correlation coefficient is:  0.8171\n",
      "This is the Confusion Matrix\n",
      "    0    1\n",
      "0  53   13\n",
      "1   7  383\n",
      "Accuracy: 0.956140350877193 \n",
      "\n",
      "- SVC:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\paulo\\OneDrive\\Ambiente de Trabalho\\LEI\\3º ano\\2º semestre\\EC\\Projeto2\\BioDegradable-MachineLearning\\BioDegradable-MachineLearning\\biodeg.ipynb Cell 25\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X32sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(model, params, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X32sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m- \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X32sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m grid\u001b[39m.\u001b[39;49mfit(X_train_option[\u001b[39m0\u001b[39;49m], y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X32sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Print the best hyperparameters and the corresponding evaluation metrics\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/paulo/OneDrive/Ambiente%20de%20Trabalho/LEI/3%C2%BA%20ano/2%C2%BA%20semestre/EC/Projeto2/BioDegradable-MachineLearning/BioDegradable-MachineLearning/biodeg.ipynb#X32sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Hyperparameters:\u001b[39m\u001b[39m\"\u001b[39m, grid\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:708\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    705\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39mfit_error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    707\u001b[0m fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m--> 708\u001b[0m test_scores \u001b[39m=\u001b[39m _score(estimator, X_test, y_test, scorer, error_score)\n\u001b[0;32m    709\u001b[0m score_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m-\u001b[39m fit_time\n\u001b[0;32m    710\u001b[0m \u001b[39mif\u001b[39;00m return_train_score:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:767\u001b[0m, in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[0;32m    765\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test)\n\u001b[0;32m    766\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 767\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test, y_test)\n\u001b[0;32m    768\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scorer, _MultimetricScorer):\n\u001b[0;32m    770\u001b[0m         \u001b[39m# If `_MultimetricScorer` raises exception, the `error_score`\u001b[39;00m\n\u001b[0;32m    771\u001b[0m         \u001b[39m# parameter is equal to \"raise\".\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_scorer.py:444\u001b[0m, in \u001b[0;36m_passthrough_scorer\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_passthrough_scorer\u001b[39m(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    443\u001b[0m     \u001b[39m\"\"\"Function that wraps estimator.score\"\"\"\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator\u001b[39m.\u001b[39mscore(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:649\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m--> 649\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py:820\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    818\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_function(X), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    819\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m    821\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py:435\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    433\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_for_predict(X)\n\u001b[0;32m    434\u001b[0m predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_predict \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dense_predict\n\u001b[1;32m--> 435\u001b[0m \u001b[39mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py:454\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    446\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    447\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mX.shape[1] = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m should be equal to \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe number of samples at training time\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m             \u001b[39m%\u001b[39m (X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_[\u001b[39m0\u001b[39m])\n\u001b[0;32m    450\u001b[0m         )\n\u001b[0;32m    452\u001b[0m svm_type \u001b[39m=\u001b[39m LIBSVM_IMPL\u001b[39m.\u001b[39mindex(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl)\n\u001b[1;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m libsvm\u001b[39m.\u001b[39;49mpredict(\n\u001b[0;32m    455\u001b[0m     X,\n\u001b[0;32m    456\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msupport_,\n\u001b[0;32m    457\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msupport_vectors_,\n\u001b[0;32m    458\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_n_support,\n\u001b[0;32m    459\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dual_coef_,\n\u001b[0;32m    460\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_intercept_,\n\u001b[0;32m    461\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_probA,\n\u001b[0;32m    462\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_probB,\n\u001b[0;32m    463\u001b[0m     svm_type\u001b[39m=\u001b[39;49msvm_type,\n\u001b[0;32m    464\u001b[0m     kernel\u001b[39m=\u001b[39;49mkernel,\n\u001b[0;32m    465\u001b[0m     degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[0;32m    466\u001b[0m     coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[0;32m    467\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma,\n\u001b[0;32m    468\u001b[0m     cache_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size,\n\u001b[0;32m    469\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# Filter out ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters to test\n",
    "logistic_params = {'C': [0.01, 0.1, 1, 10]}\n",
    "\n",
    "tree_params = {'max_depth': range(1, 11), 'min_samples_split': range(2, 11)}\n",
    "\n",
    "svm_params = {'C': [1, 10, 100, 1e3],'gamma': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "models = {\"Logistic Regression\": (LogisticRegression(), logistic_params), \n",
    "            \"Decision Tree\": (DecisionTreeClassifier(random_state=123), tree_params),\n",
    "            \"SVC\": (SVC(random_state=123), svm_params)}\n",
    "\n",
    "X_train_options = [[Xt_train, Xt_test, \"All data\"],\n",
    "                    [Xt_train_topX_corr, Xt_test_topX_corr, \"Correlation Feature selection\"], \n",
    "                    [nXt_train_stepWise_forward, nXt_test_stepWise_forward, \"Forward Stepwise Feature selection\"],\n",
    "                    [nXt_train_stepWise_backward, nXt_test_stepWise_backward, \"Backward Stepwise Feature selection\"]]\n",
    "\n",
    "for X_train_option in X_train_options:\n",
    "    print(\"---- Using \", X_train_option[2], \"----\\n\")\n",
    "    \n",
    "    # Iterate over the models and their hyperparameters\n",
    "    for model_name, (model, params) in models.items():\n",
    "        grid = GridSearchCV(model, params, cv=5)\n",
    "        print(f\"- {model_name}:\")\n",
    "    \n",
    "        grid.fit(X_train_option[0], y_train)\n",
    "        # Print the best hyperparameters and the corresponding evaluation metrics\n",
    "        print(\"Best Hyperparameters:\", grid.best_params_)\n",
    "        preds = grid.predict(X_train_option[1])\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        present_reg_statistics(y_test, preds)\n",
    "        print(\"Accuracy:\", accuracy, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
